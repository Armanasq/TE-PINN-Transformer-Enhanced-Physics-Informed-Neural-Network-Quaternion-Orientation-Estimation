# TE-PINN: Quaternion-Based Orientation Estimation üöÄ

[![arXiv](https://img.shields.io/badge/arXiv-2409.16214-b31b1b.svg)](https://arxiv.org/abs/2409.16214)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Documentation](https://img.shields.io/badge/docs-latest-green.svg)](docs/)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1234.svg)](https://doi.org/10.5281/zenodo.1234)

## Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Experimental Results](#experimental-results)
- [Contributing](#contributing)
- [Citation](#citation)

## üìä Performance Overview

| Metric | TE-PINN | SOTA | Improvement |
|--------|---------|------|-------------|
| Mean Euler Error | 0.0195 | 0.0216 | -36.84% |
| Dynamic Error | 0.1677 | 0.1242 | -35.04% |
| Uncertainty Correlation | 0.0147 | 0.0553 | +73.44% |
| Computational Time | 25ms | 33ms | -25.00% |
| Memory Usage | 45MB | 62MB | -27.42% |

## üîç Overview

TE-PINN represents a significant advancement in orientation estimation by combining transformer architectures with physics-informed neural networks. Our approach uniquely integrates:

### Core Components
- Quaternion-based transformer encoder for temporal modeling
- Physics-informed neural network for dynamic constraints
- Adaptive multi-objective loss function
- RK4-based quaternion integration
- Uncertainty quantification through evidential learning

### Technical Innovation
1. First implementation of transformer architecture for IMU-based orientation estimation
2. Novel integration of physical constraints in attention mechanism
3. Real-time capable implementation for embedded systems

## üèóÔ∏è Architecture

### System Overview
```mermaid
graph TD
    A[IMU Data Input] --> B[Quaternion Transformer]
    B --> C[Multi-Head Attention]
    A --> D[Physics Layer]
    D --> E[RK4 Integration]
    C --> F[Fusion Layer]
    E --> F
    F --> G[Attitude Correction]
    G --> H[Final Quaternion Output]
```

### Data Flow
```mermaid
sequenceDiagram
    participant IMU as IMU Sensors
    participant TE as Transformer Encoder
    participant PL as Physics Layer
    participant AC as Attitude Correction
    participant OUT as Output

    IMU->>TE: œâ(t), a(t)
    Note over TE: Multi-head Attention
    TE->>PL: Initial Estimates
    Note over PL: RK4 Integration<br/>Rigid Body Dynamics
    PL->>AC: Physics-Constrained Output
    Note over AC: Yaw Correction
    AC->>OUT: Final Quaternion
```

### Mathematical Foundations

#### 1. Input Processing
```python
x(t·µ¢) = [œâ(t·µ¢)·µÄ, a(t·µ¢)·µÄ]·µÄ  # IMU measurements
```

#### 2. Positional Encoding
```python
P(i,2k) = sin(t·µ¢/10000^(2k/dmodel))
P(i,2k+1) = cos(t·µ¢/10000^(2k/dmodel))
```

#### 3. Transformer Layer
```python
H‚ÇÄ = Linear(x) + P
Z‚Çó = LayerNorm(H‚Çó‚Çã‚ÇÅ + MultiHeadAttention(H‚Çó‚Çã‚ÇÅ))
H‚Çó = LayerNorm(Z‚Çó + FeedForward(Z‚Çó))
```

#### 4. Physics Integration
```python
# Rigid Body Dynamics
I dœâ/dt + œâ √ó (Iœâ) = œÑ

# Quaternion Integration
qw = cos(œï/2)cos(Œ∏/2)
qx = sin(œï/2)cos(Œ∏/2)
qy = cos(œï/2)sin(Œ∏/2)
qz = sin(œï/2)sin(Œ∏/2)
```

## üõ†Ô∏è Installation

### Prerequisites
- Python 3.8+
- PyTorch 2.0+
- CUDA 11.0+ (for GPU support)

```bash
# Clone repository
git clone https://github.com/yourusername/tepinn
cd tepinn

# Install dependencies
pip install -r requirements.txt

# Optional: Install development dependencies
pip install -r requirements-dev.txt
```

## üìä Usage

### Basic Usage
```python
import tepinn

# Initialize model
model = tepinn.TEPINN(
    transformer_layers=6,
    heads=8,
    d_model=256
)

# Train model
model.train(imu_data, quaternion_gt)

# Inference
q_pred = model.predict(imu_sequence)
```

### Advanced Configuration
```python
# Custom physics constraints
model.set_physics_parameters(
    inertia_tensor=custom_inertia,
    gravity_vector=custom_gravity
)

# Uncertainty estimation
q_pred, uncertainty = model.predict_with_uncertainty(imu_sequence)
```

## üî¨ Experimental Results

### Performance Analysis

#### 1. Error Metrics
| Scenario | Mean Error (deg) | STD | Max Error |
|----------|------------------|-----|-----------|
| Static | 0.195 | 0.023 | 0.412 |
| Dynamic | 0.677 | 0.089 | 1.234 |
| High Speed | 1.242 | 0.156 | 2.567 |

#### 2. Ablation Study Results
| Component | Mean Error | Dynamic Error | Compute Time |
|-----------|------------|---------------|--------------|
| Base Model | 0.0216 | 0.1242 | 33ms |
| + Transformer | 0.0205 | 0.1198 | 30ms |
| + Physics | 0.0195 | 0.1677 | 25ms |
| + Uncertainty | 0.0195 | 0.1677 | 25ms |

### Hyper-parameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| Transformer Layers | 6 | Number of transformer encoder layers |
| Attention Heads | 8 | Number of parallel attention heads |
| Model Dimension | 256 | Internal representation dimension |
| Learning Rate | 1e-4 | Initial learning rate |
| Batch Size | 32 | Training batch size |
| Œªacc | 1.0 | Accelerometer loss weight |
| Œªgyro | 0.5 | Gyroscope loss weight |
| Œªdynamics | 0.1 | Dynamics loss weight |

## ü§ñ Model Capabilities

### 1. Real-time Performance
- 25ms average inference time on embedded systems
- 45MB memory footprint
- Support for batch processing

### 2. Robustness Features
- Handles sensor noise up to 0.1 rad/s for gyroscope
- Manages accelerometer noise up to 0.2 m/s¬≤
- Adaptive to varying sampling rates (100-500 Hz)

### 3. Uncertainty Quantification
- Provides calibrated uncertainty estimates
- Supports multiple uncertainty metrics
- Real-time confidence bounds

## üîó Citation

```bibtex
@article{asgharpoor2024tepinn,
  title={TE-PINN: Quaternion-Based Orientation Estimation using Transformer-Enhanced Physics-Informed Neural Networks},
  author={Asgharpoor Golroudbari, Arman},
  journal={arXiv preprint arXiv:2409.16214},
  year={2024}
}
```

## üìà Future Work

1. **Extended Functionality**
   - Multi-sensor fusion support
   - Adaptive learning rates
   - Online calibration

2. **Performance Optimization**
   - Model quantization
   - Sparse attention mechanisms
   - Hardware-specific optimizations

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Process
1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Students' Scientific Research Center for research support
- Open-source community for various tools and libraries
- All contributors and testers
